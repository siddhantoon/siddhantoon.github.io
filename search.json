[
  {
    "objectID": "posts/why-positional-encoding/index.html",
    "href": "posts/why-positional-encoding/index.html",
    "title": "what/why/how positional encodings?",
    "section": "",
    "text": "Because we are working with sequential data and a parallel model. We need to have some representation of the position of items in a sequence. Text is sequential, position matters simply. Model is parallel - transformers architecture has no clue that different tokens are far or close to one another, all tokens are same to this model. Unlike RNN family where processing was sequential so position was encoded in the way we processed. So we need the model to know that there is positions of different tokens involved here which is needed to understand the patterns."
  },
  {
    "objectID": "posts/why-positional-encoding/index.html#why-positional-encodings",
    "href": "posts/why-positional-encoding/index.html#why-positional-encodings",
    "title": "what/why/how positional encodings?",
    "section": "",
    "text": "Because we are working with sequential data and a parallel model. We need to have some representation of the position of items in a sequence. Text is sequential, position matters simply. Model is parallel - transformers architecture has no clue that different tokens are far or close to one another, all tokens are same to this model. Unlike RNN family where processing was sequential so position was encoded in the way we processed. So we need the model to know that there is positions of different tokens involved here which is needed to understand the patterns."
  },
  {
    "objectID": "posts/why-positional-encoding/index.html#absolute",
    "href": "posts/why-positional-encoding/index.html#absolute",
    "title": "what/why/how positional encodings?",
    "section": "absolute",
    "text": "absolute\nThe very simple thing you can think of is using the absolute position embeddings - doing something like this is position 1,2,3,â€¦ convert it to embedding or encode it with the tokens somehow. This is the approach used in BERT - this works but how do you scale - you have learned embeddings for upto some length which the model learned - but this is not generalized. Even theoretically you cannot scale it because these are learned embeddings - No relative distance - position 50 is a unique thing, 51 is a unique thing. This doesnâ€™t tell the model that 51 is adjacent to 50."
  },
  {
    "objectID": "posts/why-positional-encoding/index.html#sinusoidal",
    "href": "posts/why-positional-encoding/index.html#sinusoidal",
    "title": "what/why/how positional encodings?",
    "section": "sinusoidal",
    "text": "sinusoidal\neven in the OG â€œAttentionâ€ paper they used the Sinusoidal encodings\n\nwhat/why?\n\nWe are using Sin wave to encode the position Sin(Ï‰t) where t is position, something like Sin(Ï‰* 1), Sin(Ï‰* 2), Sin(Ï‰* 3),â€¦.Sin(Ï‰* t) -&gt; this creates like a wave pattern, each position gets a different value, depending on Ï‰ we may get repeat values. So even Ï‰ is different for each dimension of a token.\nThis is directly added to the token embedding\nNearby embeddings will have similar values - 10 & 11 position vectors are less different that 10 and 50. Giving some idea about the position\nWe can extrapolate it to any amount of length or positionâ€¦ at least in theory\n\n\n\nhow/implementation?\n We have an alternating Sin Cos waves with varying Ï‰ or speed of change - dimensions with low Ï‰ -&gt; less change with each position -&gt; they can store positional differences of longer sequences - dimensions with high Ï‰ -&gt; high change with each position -&gt; they can store positional differences of longer sequences Initial dim Ï‰ is high, later Ï‰ is smaller Okay so how do we code it?  \n\n\nvectorized is good\nBefore coming to code, we use a different formula for the divterm we take sin and cos of this term\n\\[\n\\text{divterm}= \\frac{1}{10000^{\\frac{2k}{d}}} \\\\\n\\] \\[\n\\text{divterm}= e^{\\frac{-2k}{d}log(10000)}\n\\]\nWhy this form?? - itâ€™s easier/faster to vectorize. - Vectorized computations are way faster - do torch.arange for k and the rest is constant. Why faster?? - I went to find itâ€™s answer & you should too, itâ€™s really interesting. Also search for why exactly vectorization is faster than loops on a GPU. How it launches kernels n stuff.\nSo below is the code, freaking few lines of code but needs a lot of understanding -\nT,C = 10, 32 # Time steps, Channels\n\npos = torch.arange(T, dtype=torch.float32).view(T, 1)\n\ndivterm = torch.exp(torch.arange(0, C, 2, dtype=torch.float32) * -(torch.log(torch.tensor(10000.0)) / C)).unsqueeze(0)\n\n# (pos@divterm).shape\n\npe = torch.zeros(T, C)\n\npe[:, 0::2] = torch.sin(pos @ divterm)\n\npe[:, 1::2] = torch.cos(pos @ divterm)\n\n\nwhy not sinusoidal?\n\nWhen tested, these did not work really well in extrapolation."
  },
  {
    "objectID": "posts/why-positional-encoding/index.html#rotary-positional-encoding-rope",
    "href": "posts/why-positional-encoding/index.html#rotary-positional-encoding-rope",
    "title": "what/why/how positional encodings?",
    "section": "Rotary Positional Encoding RoPE",
    "text": "Rotary Positional Encoding RoPE\nWhen people started experimenting with new positional encodings, this paper was published in 2021, it took inspiration from the original sinusoidal encodings, with a different form & approach\n\nwhat/why?\n\nThe idea is to take the vector of a token, simply rotate it\nif the position is 2 rotate it with \\(2.\\theta\\), if position is 3 rotate it by \\(3.\\theta\\)\nSounds simple, this should be sufficient to encode the positional information, as we are rotating the vector according to itâ€™s info \nthis is applied to the query and key of token embeddings inside attention before calculation of the self-attention weights in every attention block\nnot to the initial token embeddings once as done in the OG attention paper\n\n\n\nhow/implementation?\nThe rotation matrix above is of 2D, but actually we have a lot more dimensions than 2. What we do is take a pair of dimensions and apply rotation to them, so a kind of rotation in higher dimension. this is block rotation matrix   The \\(\\theta\\) varies as - same as the sinusoidal encodings, \\[\n\\theta_i = 10000^{-\\frac{2k}{d}}\n\\] \n\n\ncode\ndef get_rotary_embedding(dim, seq_len):\nÂ  Â  # Base: 10000 as used in original paper\nÂ  Â  inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim)) # N^(-2i/d) where i = [0,1,2,..d/2]\nÂ  Â  # Create position indices\nÂ  Â  positions = torch.arange(seq_len).float() # This is going to act as n in n theta\nÂ  Â  # Outer product (pos Ã— freq)\nÂ  Â  freqs = torch.outer(positions, inv_freq) Â # (seq_len, dim // 2)\nÂ  Â  # sin and cos for rotation\nÂ  Â  # (seq_len, dim // 2) -&gt; (seq_len, dim)\nÂ  Â  freqs = torch.repeat_interleave(freqs, 2, dim=-1)\nÂ  Â  sin = freqs.sin() Â # (seq_len, dim)\nÂ  Â  cos = freqs.cos() Â # (seq_len, dim)\nÂ  Â  return sin, cos\n\ndef invert_twin(x):\nÂ  Â  \"\"\"[1,2,3,4,5,6,] -&gt; [-2, 1, -4, 3, -6, 5] for the last dim\"\"\"\nÂ  Â  x_odd = x[..., ::2]\nÂ  Â  x_even = x[..., 1::2]\nÂ  Â  return torch.stack((-x_even, x_odd), dim=-1).flatten(-2)\n\ndef apply_rotary(q, k, sin, cos):\nÂ  Â  \"\"\"\nÂ  Â  Args:\nÂ  Â  Â  Â  q: [BS, seq_len, dim]\nÂ  Â  Â  Â  k: [BS, seq_len, dim]\nÂ  Â  Â  Â  sin: [seq_len, dim]\nÂ  Â  Â  Â  cos: [seq_len, dim]\nÂ  Â  \"\"\"\nÂ  Â  q = q*cos + invert_twin(q)*sin\nÂ  Â  k = k*cos + invert_twin(k)*sin\nÂ  Â  return q,k\n\n\nextrapolation?\n\nthis is applied on the query & keys not values, thus position encoded only in attention values. less overfitting to specific position values\nthis also works on the relative position not the absolute position of the tokens, and the distance between token matters.\nevidence suggests it is better than sinusoidal encodings\n\n\n\nhow relative position?\n\nso the rotation \\(\\theta\\) not just encode the position of that token, how it is applied actually tells us\nthat weâ€™re encoding the distance between two tokens.\nwhen the query and key embedding interact in self attention, their relative distance matters"
  },
  {
    "objectID": "posts/why-positional-encoding/index.html#conclusion",
    "href": "posts/why-positional-encoding/index.html#conclusion",
    "title": "what/why/how positional encodings?",
    "section": "conclusion",
    "text": "conclusion\nthis is most widely used way to encode positional info, there are ways to improve upon it\ntopics iâ€™ll be exploring later -\n\nPosition Interpolation\nYaRN\nNTK-Aware scaling"
  },
  {
    "objectID": "posts/paligemma-vision-language-basics/index.html",
    "href": "posts/paligemma-vision-language-basics/index.html",
    "title": "PaliGemma from scratch - Vision Language model basics",
    "section": "",
    "text": "We have to make a model which can understand images. A basic vision model.\nWe will have a vision model and a main language model, because we need a language model to basically interact in language."
  },
  {
    "objectID": "posts/paligemma-vision-language-basics/index.html#rms-norm---simply-rms-norm",
    "href": "posts/paligemma-vision-language-basics/index.html#rms-norm---simply-rms-norm",
    "title": "PaliGemma from scratch - Vision Language model basics",
    "section": "RMS Norm - Simply RMS norm",
    "text": "RMS Norm - Simply RMS norm\n\nTo reduce internal covariate shift - Batch norm was introduced\nThen we started using Layer Norm because Batch norm was kind of mixing the statistics for different items with each other which is kinda weird.\nLayer norm is to center the distribution of outputs of a layer around 0 (with 1 std).\nRMSNorm says why do we need to center it around 0 - it should be centered around any value just being centered is important\nSo they calculate the RMS of a layer - and \\[\n  \\bar{a}_i = \\frac{a_i}{\\mathrm{RMS}(a)}\\, g_i,\n  \\qquad \\text{where} \\qquad\n  \\mathrm{RMS}(a) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} a_i^2 }\n  \\]"
  },
  {
    "objectID": "posts/paligemma-vision-language-basics/index.html#group-query-attention",
    "href": "posts/paligemma-vision-language-basics/index.html#group-query-attention",
    "title": "PaliGemma from scratch - Vision Language model basics",
    "section": "Group Query Attention",
    "text": "Group Query Attention\n\n\nThe implementation is here"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Siddhant's Blog",
    "section": "",
    "text": "Hi, Iâ€™m Siddhant ğŸ‘‹ Iâ€™m a Machine Learning Engineer who loves building practical AI systems and sharing what I learn along the way.\nThis blog is my space to learn in public â€” a place where I document concepts, experiments, failures, and â€œahaâ€ moments. If youâ€™re into Deep Learning, LLMs, NLP, Python, or building systems like RAG and search, youâ€™ll feel at home here."
  },
  {
    "objectID": "about.html#what-i-do-right-now",
    "href": "about.html#what-i-do-right-now",
    "title": "Siddhant's Blog",
    "section": "What I do right now",
    "text": "What I do right now\nI currently work as a Machine Learning Engineer at OMEGA Labs, where I focus on LLM evaluation and incentive design in a decentralized AI setting. A big part of my work is building evaluation pipelines, finding issues in existing scoring, and designing mechanisms that reward genuinely better models â€” not just â€œgood-looking metrics.â€"
  },
  {
    "objectID": "about.html#my-background-and-how-i-got-here",
    "href": "about.html#my-background-and-how-i-got-here",
    "title": "Siddhant's Blog",
    "section": "My background (and how I got here)",
    "text": "My background (and how I got here)\nOver the past 3+ years, Iâ€™ve worked deeply in the Document AI space â€” building and scaling pipelines that extract, structure, and validate information from messy, real-world documents.\nPreviously, I was a Data Scientist II at Wolters Kluwer (Jun 2023 â€“ Jun 2025), where I worked across multiple products and problem types, including:\n\nRAG + search systems: Built a hybrid approach combining historical user queries with rule-based results to improve search relevance.\nInformation extraction from PDFs: Integrated AWS Textract and later added LLM-based structured extraction (few-shot + structured outputs), with a workflow that made onboarding faster and more configurable for operations teams.\nRule clustering & reuse: Found redundancy in large rule sets, clustered similar rules (with GPT-4o-assisted logic), and built a search workbench on Azure AI Search to help users navigate and reuse them.\nDocument classification: Added page-level classification using DONUT to reduce manual work across multi-state form variations.\n\nI started even earlier as a Data Science Intern at Wolters Kluwer (Jul 2022 â€“ May 2023), and also worked as a Data Science Intern at Mowito, where I explored object detection approaches on SKU/image datasets."
  },
  {
    "objectID": "about.html#before-ai-robotics-and-computer-vision",
    "href": "about.html#before-ai-robotics-and-computer-vision",
    "title": "Siddhant's Blog",
    "section": "Before AI: robotics and computer vision ğŸ¤–",
    "text": "Before AI: robotics and computer vision ğŸ¤–\nBefore I went all-in on LLMs and DocAI, I was deep into robotics â€” building, experimenting, and learning through hands-on projects. That naturally pulled me into computer vision, where I worked with both classical techniques (OpenCV, edge detection, Hough transforms) and deep learning models like YOLO.\nIâ€™ve also built projects like an image caption generator, mask detection, a sign-detection robot, and explored ideas like variational autoencoders for representation learning."
  },
  {
    "objectID": "about.html#what-im-learning-these-days",
    "href": "about.html#what-im-learning-these-days",
    "title": "Siddhant's Blog",
    "section": "What Iâ€™m learning these days",
    "text": "What Iâ€™m learning these days\nRight now, Iâ€™m focused on going deeper into:\n\nTransformers & LLM architecture (not just using them â€” understanding them)\nEvaluation (what metrics lie, what signals matter, how to stress-test systems)\nRAG & smart search in real products (latency, observability, failure modes, cost)\nKaggle competitions to stay sharp and hands-on"
  },
  {
    "objectID": "about.html#my-toolkit",
    "href": "about.html#my-toolkit",
    "title": "Siddhant's Blog",
    "section": "My toolkit",
    "text": "My toolkit\nI work mostly in Python, and Iâ€™m comfortable taking projects from experimentation â†’ pipelines â†’ deployment.\nSome tools I regularly use: PyTorch, Hugging Face Transformers, LangChain, FastAPI/Django, Docker, SQL, Azure AI Search / Azure Doc Intelligence, AWS Textract, vector DBs (e.g., ChromaDB), and MLOps tooling like MLflow / Weights & Biases."
  },
  {
    "objectID": "about.html#a-few-highlights-for-quick-context",
    "href": "about.html#a-few-highlights-for-quick-context",
    "title": "Siddhant's Blog",
    "section": "A few highlights (for quick context)",
    "text": "A few highlights (for quick context)\n\nB.Tech (ECE), IIIT Pune â€” May 2023\n2nd place â€” DOXA AI Hackathon (Mar 2025)\n2nd place â€” ROS Hackathon (IRC, Jul 2021)\nMentorship under RigBetel Labs + founded the Robotics Club at IIIT Pune"
  },
  {
    "objectID": "about.html#lets-connect",
    "href": "about.html#lets-connect",
    "title": "Siddhant's Blog",
    "section": "Letâ€™s connect",
    "text": "Letâ€™s connect\nIf youâ€™re curious about NLP, LLMs, Document Intelligence, RAG, or building scalable AI systems, Iâ€™d love to connect.\n\nğŸ“© Email: siddhantoon@gmail.com\nğŸ”— You can also find my GitHub / LinkedIn via the links on this site.\n\n\nFreelance / consulting\nIâ€™m available for freelance or consulting in:\n\nDocument Intelligence (extraction, classification, validation)\nRAG + search systems (retrieval, evaluation, production readiness)\nML engineering (pipelines, deployment, experimentation workflows)\n\n\n\nKaggle / hackathons\nIf youâ€™re up for teaming on Kaggle or hackathons, ping me â€” always down to learn together ğŸš€"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Siddhant's Blog",
    "section": "",
    "text": "Hi, Iâ€™m Siddhant ğŸ‘‹\nThis blog is my space to learn in public â€” to share what I discover and connect with others on a similar path. If youâ€™re into (or wanna get into) Deep Learning, LLMs, or Python, youâ€™re in the right place.\nI currently work as a Machine Learning Engineer at OMEGA Labs. Over the past 3 years, Iâ€™ve been building and scaling intelligent pipelines in the Document AI space â€” processing unstructured documents. Recently, building systems around Retrieval-Augmented Generation (RAG) and smart search.\nOn the side, Iâ€™ve freelanced as a Python dev and AI engineer â€” sharpening my programming skills and gaining a broad view of how AI projects come together end to end.\nBefore AI fever, I was deep into robotics ğŸ¤– â€” experimenting in simulation and occasionally with real hardware (when someone sponsored). That path eventually led me into computer vision, where I worked with both classical algorithms and deep nn models like YOLO, ResNets, and GANs.\nThese days, Iâ€™m focused on learning transformers and LLM architectures in depth. Alongside, exploring Kaggle competitions to stay hands-on and push my thinking through experimentation.\n\nIf youâ€™re curious about NLP, LLMs, document intelligence, or building scalable AI systems â€” Iâ€™d love to connect.\nğŸ‘‰ Available for freelance or consulting in AI/ML, Document Intelligence, or RAG-based systems â€” feel free to reach out.\nğŸš€ Up for teaming on Kaggle or hackathons? Ping me on Discord â€” always down to learn together.\n\n\nBlogs\n\n\n\n\n\nPaliGemma from scratch - Vision Language model basics\n\n\n\n\n\n\n\n\nJan 8, 2026\n\n\n\n\n\n\n\nwhat/why/how positional encodings?\n\n\n\n\n\n\n\n\nNov 29, 2025\n\n\n\n\n\n\n\nThe Loss That Learned Everything\n\n\nAn exploration of loss functions and their role in machine learning optimization\n\n\n\n\n\nApr 4, 2025\n\n\n\n\n\nNo matching items\n\n\nShow More â†’"
  },
  {
    "objectID": "posts/the-loss-that-learned-everything/index.html",
    "href": "posts/the-loss-that-learned-everything/index.html",
    "title": "The Loss That Learned Everything",
    "section": "",
    "text": "Redirecting to the full articleâ€¦\nIf you are not redirected automatically, click here to read the article."
  }
]